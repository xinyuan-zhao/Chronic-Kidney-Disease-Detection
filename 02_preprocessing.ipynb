{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "796c5185-d56a-4062-a8e9-3e9ce660246b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We will:\n",
    "1. Load the cleaned dataset  \n",
    "2. Preprocess:\n",
    "   - Handle missing values\n",
    "   - Encode categorical features\n",
    "   - Scale numerical features\n",
    "3. Split into train/test sets\n",
    "4. Train a basic model\n",
    "5. Use **MLflow** to track:\n",
    "   - Parameters\n",
    "   - Metrics\n",
    "   - Model artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20a744c9-4a7b-4ecc-8092-b2cf974a56ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Load the cleaned data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "141b325f-b2ae-43c7-89e0-f3d4eca16291",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "file_path = \"/dbfs/FileStore/tables/kidney_disease_cleaned.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df['classification'] = df['classification'].str.strip().str.lower()\n",
    "df.drop('id', axis=1, inplace=True, errors='ignore')  # drop if not already dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11c7026c-693a-4d1d-a0ae-b256acd7203b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Define Features & Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88756ed6-c235-4d11-9082-41a91f238a7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop(\"classification\", axis=1)\n",
    "y = df[\"classification\"].map({'ckd': 1, 'notckd': 0})  # binary target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66829711-abbc-4670-97e1-f07d50aba2ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Create Preprocessing Pipeline\n",
    "We'll use:\n",
    "- **SimpleImputer** for missing values\n",
    "- **StandardScaler** for numeric\n",
    "- **OneHotEncoder** for categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be47f614-6576-421c-8201-9c07ec2fb696",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Identify column types\n",
    "num_cols = X.select_dtypes(include=[\"float64\", \"int64\"]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "# Pipelines\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_cols),\n",
    "    (\"cat\", cat_pipeline, cat_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd1f07f1-0a6f-4e4c-b58d-0f172ea1eb0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93ecacf7-e843-4866-a255-bb10ea92d40a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ba243d9-8ac5-43b1-96de-349e3660aa5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Build Full Pipeline + Model\n",
    "Weâ€™ll use `RandomForestClassifier` as a start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57758179-e397-4c49-8780-88d2fdc5f60f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_pipeline = Pipeline([\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39d3779e-b0a0-458a-9a33-748eb44a06a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Train Model + Track with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c79b78a9-01a8-4434-b556-a44039827a1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "mlflow.set_experiment(\"/Users/zhao.xinyuan@northeastern.edu/ckd-mlops\")  # set this to your Databricks email path\n",
    "\n",
    "with mlflow.start_run():\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "    preds = model_pipeline.predict(X_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    clf_report = classification_report(y_test, preds, output_dict=True)\n",
    "\n",
    "    # Log params, metrics\n",
    "    mlflow.log_param(\"model_type\", \"RandomForest\")\n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "    mlflow.log_metrics({f\"f1_{k}\": v[\"f1-score\"] for k, v in clf_report.items() if k in [\"0\", \"1\"]})\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(model_pipeline, \"ckd_rf_model\")\n",
    "\n",
    "    print(\"Run logged with MLflow ðŸŽ‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "190b22dc-c747-484f-bec5-f427af92532f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## What Youâ€™ll Learn/Showcase\n",
    "\n",
    "| Feature | Why Itâ€™s Important |\n",
    "|--------|--------------------|\n",
    "| `Pipeline` | Clean, repeatable ML process |\n",
    "| `ColumnTransformer` | Real-world data handling (numeric + categorical) |\n",
    "| `MLflow` | MLOps logging = critical for Sanofi role |\n",
    "| `RandomForestClassifier` | A solid baseline ML model |\n",
    "| `classification_report` | Communicates performance clearly |\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Once you're done:\n",
    "1. Iâ€™ll help you move to `03_model_selection_and_tuning.ipynb` for **hyperparameter tuning** and **model comparison**.\n",
    "2. Then we can add **Streamlit** or **MLflow Serving Endpoint** if you want to demo!\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_preprocessing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
